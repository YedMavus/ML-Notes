<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.86.1">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Week 2 Lecture 2: Linear Regression &#183; ML Notes</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.png>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
</head>
<body class="theme-base-0d layout-reverse">
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a>
<p class=lead>
Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://yedmavus.github.io/MLNotes/>Home</a> </li>
<li><a href=https://github.com/YedMavus/> Github </a></li><li><a href=https://www.linkedin.com/in/suvam-dey/> LinkedIn </a></li>
</ul>
</nav>
<p>&copy; 2021. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<h1>Week 2 Lecture 2: Linear Regression</h1>
<time datetime=2021-08-02T00:00:00Z class=post-date>Mon, Aug 2, 2021</time>
<p>\( \usepackage{graphicx} \)
\( \graphicspath{ {./images/} } \)</p>
<h1 id=week-2-lecture-2-linear-regression>Week 2 Lecture 2: Linear Regression</h1>
<h2 id=regression-in-x-and-y>Regression in X and Y</h2>
<h3 id=firstly-for-regression-y-is-continious>Firstly for <strong>regression</strong> Y is continious</h3>
<p>X \( \rightarrow \) Y</p>
<ul>
<li>Single Regression: X consists of only 1 feature
<ul>
<li>Find fucntion given arbitrary X, that can predict Y with minimum error</li>
<li>In <strong>Linear Regression</strong> this output Y function happens to be a line, trained based on an existing set of (X,Y)</li>
</ul>
</li>
<li>Multiple Regression: Multiple features in <strong>X</strong></li>
</ul>
<h2 id=linear-regression>Linear Regression</h2>
<h3 id=simple-linear-regression>Simple Linear Regression:</h3>
<h4 id=-y--theta-_0--theta-_-1-x->\( Y = \theta _0 + \theta _ 1 X \)</h4>
<p>Obviously, \( \theta _0 \) is the Y axis intercept, while \( \theta _ 1 \) is the population slope.</p>
<h3 id=multiple-linear-regression>Multiple Linear Regression</h3>
<h4 id=-y--theta-_0--theta-_-1-x--theta-_-2-x_2----theta-_-p-x_p->\( Y = \theta _0 + \theta _ 1 X + \theta _ 2 X_2 + &mldr; + \theta _ p X_p \)</h4>
<p>Thus \( \theta _i \) where i>0 is the slope of the i^th variable X_i, and \( \theta _0 \) is the average value of Y if all X are made 0.</p>
<p>These are the expected values, in actuality, both LR in single and multiple have an extra random error term \( \epsilon \) added at the end.</p>
<p>We try to find \( \theta _0 , \theta _ 1 , \theta _ 2 , &mldr; , \theta _ p \) such that it fits the given data closely.</p>
<h2 id=assumptions-in-lr>Assumptions in LR</h2>
<p>Consider <strong>simple LR</strong>: \( Y = \theta _0 + \theta _ 1 X + \epsilon \)</p>
<p>Let E(p) denote expectation of P.</p>
<p>The assumption we make are</p>
<ol>
<li>Dataset is linear</li>
<li>E( \(\epsilon _i \) ) = 0 [ie mean of random errors is 0]</li>
<li>\(\sigma \) ( \(\epsilon _i \) ) = \(\sigma _0 \) [Some std dev exists]</li>
<li>Errors are independent of each other</li>
<li>Errors are normally distributed</li>
</ol>
<p>This kind of noise is called Gaussian noise</p>
<h2 id=fitting>Fitting</h2>
<p>We try to find \( \theta _0 , \theta _ 1 , \theta _ 2 , &mldr; , \theta _ p \) such that it fits the given data closely, such that the sum of squares of vertical distances between data points and the line is the least [ called <strong>Least Squares regression line</strong>, and this is a unique line ]</p>
<p>Given training points \(x_i , y_i ) \)</p>
<p>Sum of sq errors is given by \( \Sigma ( y_i - ( \theta _0 + \theta _1 x_1 )^2 \)</p>
<p>And we want to minimise this by changing \( \theta _i \)
This can be done by taking pratial derivative of the function wrt the coefficients \( \theta \) and setting them to 0</p>
<p>\( \theta _1 = \frac{n\Sigma xy - \Sigma x \Sigma y}{ n \Sigma x^2 - (\Sigma x)^2} \)</p>
<p>\( \theta _0 = \frac{\Sigma y - \theta _1 \Sigma x }{ n} \)</p>
</div>
</main>
</body>
</html>