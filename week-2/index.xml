<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week-2s on ML Notes</title><link>https://suvam.me/MLNotes/week-2/</link><description>Recent content in Week-2s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 07 Aug 2021 00:03:00 +0000</lastBuildDate><atom:link href="https://suvam.me/MLNotes/week-2/index.xml" rel="self" type="application/rss+xml"/><item><title>Week 2 Lecture 3: Learning Decision Tree</title><link>https://suvam.me/MLNotes/week-2/w2l3/</link><pubDate>Sat, 07 Aug 2021 00:03:00 +0000</pubDate><guid>https://suvam.me/MLNotes/week-2/w2l3/</guid><description>Week 2 Lecture 3: Learning Decision Tree https://www.kaggle.com/dansbecker/underfitting-and-overfitting</description></item><item><title>Week 2 Lecture 2: Decision Tree</title><link>https://suvam.me/MLNotes/week-2/w2l2/</link><pubDate>Mon, 02 Aug 2021 00:03:00 +0000</pubDate><guid>https://suvam.me/MLNotes/week-2/w2l2/</guid><description>Week 2 Lecture 2: Decision Tree Decision Trees Its a tree structured classifier consisting of
Decision nodes - specify a choice or a test that tells you to which subsequent branch to take Has 2, or even 3 branches Leaf Nodes - dont have any subsequent chain(s), indicates the classification or value of example Used mainly to visualise algorithms.</description></item><item><title>Week 2 Lecture 1: Linear Regression</title><link>https://suvam.me/MLNotes/week-2/w2l1/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://suvam.me/MLNotes/week-2/w2l1/</guid><description>Week 2 Lecture 1: Linear Regression Regression in X and Y Firstly for regression Y is continious X \( \rightarrow \) Y
Single Regression: X consists of only 1 feature Find fucntion given arbitrary X, that can predict Y with minimum error In Linear Regression this output Y function happens to be a line, trained based on an existing set of (X,Y) Multiple Regression: Multiple features in X Linear Regression Simple Linear Regression: \( Y = \theta _0 + \theta _ 1 X \) Obviously, \( \theta _0 \) is the Y axis intercept, while \( \theta _ 1 \) is the population slope.</description></item></channel></rss>