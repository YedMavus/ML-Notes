<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week-2s on ML Notes</title><link>https://yedmavus.github.io/MLNotes/week-2/</link><description>Recent content in Week-2s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 02 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/week-2/index.xml" rel="self" type="application/rss+xml"/><item><title>Week 2 Lecture 2: Linear Regression</title><link>https://yedmavus.github.io/MLNotes/week-2/w2l1/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-2/w2l1/</guid><description>\( \usepackage{graphicx} \) \( \graphicspath{ {./images/} } \)
Week 2 Lecture 2: Linear Regression Regression in X and Y Firstly for regression Y is continious X \( \rightarrow \) Y
Single Regression: X consists of only 1 feature Find fucntion given arbitrary X, that can predict Y with minimum error In Linear Regression this output Y function happens to be a line, trained based on an existing set of (X,Y) Multiple Regression: Multiple features in X Linear Regression Simple Linear Regression: \( Y = \beta _0 + \beta _ 1 X \) Obviously, \( \beta _0 \) is the Y axis intercept, while \( \beta _ 1 \) is the population slope.</description></item></channel></rss>