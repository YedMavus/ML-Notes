<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.87.0">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>ML Notes</title>
<meta name=description content="Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey">
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.png>
<link href=https://yedmavus.github.io/MLNotes/index.xml rel=alternate type=application/rss+xml title="ML Notes">
</head>
<body class="theme-base-0d layout-reverse">
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a>
<p class=lead>
Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://yedmavus.github.io/MLNotes/>Home</a> </li>
<li><a href=https://github.com/YedMavus/> Github </a></li><li><a href=https://www.linkedin.com/in/suvam-dey/> LinkedIn </a></li><li><a href=https://yedmavus.github.io/MLNotes/category/> PageHome </a></li>
</ul>
</nav>
<p>&copy; 2021. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=posts>
<article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-4/w4l1/>Week 4 Lecture 1: Bayesian Learning; An Introduction</a>
</h1>
<time datetime=2021-08-25T00:00:00Z class=post-date>Wed, Aug 25, 2021</time>
Bayesian Probability Talks about probability interpretation based on partial beliefs.
Bayesian Estimation Calculates validity of a proposition: based on
Prior Estimate of its prob New relevent evidence Baye&rsquo;s Theorem \( P(h|D) = \frac{P(D|h)P(h)}{P(D)} \)
How to apply this in ML? MAP [ Maximum A Posteriori] Hypothesis
\( h_{MAP} = argmax P(h|D) \) \( h \epsilon H \)
\( = argmax(P(D|h)P(h)) \)
Maximum Likely [ML] Hyp: In cases where all the hypothesis are equally probable.
<div class=read-more-link>
<a href=/MLNotes/week-4/w4l1/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week3/w3l4/>Collaborative Filtering</a>
</h1>
<time datetime=2021-08-24T00:00:00Z class=post-date>Tue, Aug 24, 2021</time>
Recommender Systems Item Recomendation Rating Prediction Defn: We have a set of users U and a set of items S
P is a function from
\( P : U \bigcross S \rightarrow R \)
So we learn P from data, and based on P we predict the utility value of each item to each user.
Content based RS: Rating prediction based on content of current and previous item (like in YouTube) Collaborative or Content Based RS For a user find similar users [based on past data] and if those similar users have given a rating of an item, predict that [kind of like Knn] Item Based [Alternative to Collaborative]
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week3/w3l3/>Feature Extraction</a>
</h1>
<time datetime=2021-08-24T00:00:00Z class=post-date>Tue, Aug 24, 2021</time>
We find a projection matrix \( w \) such that \( \vec z = w^T \vec X \)
Also features have to have large variences
Principal Components Find w (eigenvecotrs) to get max varience, then lecond max , 3rd max etc till N dimensions are fulfilled
More on PCA
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week3/w3l1_2/>KNN</a>
</h1>
<time datetime=2021-08-24T00:00:00Z class=post-date>Tue, Aug 24, 2021</time>
Feature Reduction For K-Nearest Neighbours More features reduce accuracy. More info != more discriminative power!
This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn) Moreover they may have redundant features, as we have limited computational resources.
So we use 2 methods to reduce this:
Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects Feature Extraction: Transforms or projects the existing features to a dimention m &lt; n (original no.
<div class=read-more-link>
<a href=/MLNotes/week3/w3l1_2/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/category/>Note Home</a>
</h1>
<time datetime=2021-08-21T00:00:00Z class=post-date>Sat, Aug 21, 2021</time>
Weekly Progression:
Week-1 Lecture 1 Week-1 Lecture 2 Week-1 Lecture 3 Week-1 Lecture 4 Week-1 Lecture 5 Week-2 Lecture 1 Week-2 Lecture 2 Week-2 Lecture 3 Week-3 Lecture 1,2 NeuralNetworks
Blue1Brown-1 Notations in Neural Networks Supremely Better Resources than this crap:
http://3b1b.co/neural-networks http://colah.github.io/ http://neuralnetworksanddeeplearning.com/chap1.html
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/nn/nn-notations/>Notation in Neural Networks</a>
</h1>
<time datetime=2021-08-17T00:00:00Z class=post-date>Tue, Aug 17, 2021</time>
Notation in Neural Networks Reference Video: https://www.3blue1brown.com/lessons/backpropagation-calculus
a usually denotes activation layer node, w - weights, b - bias To imply which layer the particular node is in, we use the superscript. If the node is in layer L, it&rsquo;s activation may be denoted by \( a^{(L)} \) while that of the one before may be \( a^{(L-1)} \) .
If the desider output is denoted by y, for that particular activation node, it&rsquo;s cost is \( C_0 ( \cdots ) = ( a ^{(L)} - y^2 ) \)
<div class=read-more-link>
<a href=/MLNotes/nn/nn-notations/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/nn/3b1bnn1/>3b1bNN1</a>
</h1>
<time datetime=2021-08-08T00:00:00Z class=post-date>Sun, Aug 8, 2021</time>
But What is a Neural Network Here we take example of handwriting recognition to detect Numbers Couple of layers Each layer contains nodes that take care of a part of the number. So each of these functions sort fo drives or pushes the output towards on enumber, which on adding up end up predicting one particular number at the end.
Basically it is calculating weighted sum, while squishing the whole number line in between 0-1, so that the numbers are easier to process, and is anologous to probability.
<div class=read-more-link>
<a href=/MLNotes/nn/3b1bnn1/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-2/w2l3/>Week 2 Lecture 3: Learning Decision Tree</a>
</h1>
<time datetime=2021-08-07T00:03:00Z class=post-date>Sat, Aug 7, 2021</time>
Week 2 Lecture 3: Learning Decision Tree https://www.kaggle.com/dansbecker/underfitting-and-overfitting
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-2/w2l2/>Week 2 Lecture 2: Decision Tree</a>
</h1>
<time datetime=2021-08-02T00:03:00Z class=post-date>Mon, Aug 2, 2021</time>
Week 2 Lecture 2: Decision Tree Decision Trees Its a tree structured classifier consisting of
Decision nodes - specify a choice or a test that tells you to which subsequent branch to take Has 2, or even 3 branches Leaf Nodes - dont have any subsequent chain(s), indicates the classification or value of example Used mainly to visualise algorithms.
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-2/w2l1/>Week 2 Lecture 1: Linear Regression</a>
</h1>
<time datetime=2021-08-02T00:00:00Z class=post-date>Mon, Aug 2, 2021</time>
Week 2 Lecture 1: Linear Regression Regression in X and Y Firstly for regression Y is continious X \( \rightarrow \) Y
Single Regression: X consists of only 1 feature Find fucntion given arbitrary X, that can predict Y with minimum error In Linear Regression this output Y function happens to be a line, trained based on an existing set of (X,Y) Multiple Regression: Multiple features in X Linear Regression Simple Linear Regression: \( Y = \theta _0 + \theta _ 1 X \) Obviously, \( \theta _0 \) is the Y axis intercept, while \( \theta _ 1 \) is the population slope.
<div class=read-more-link>
<a href=/MLNotes/week-2/w2l1/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/w1lec5/>Week 1 Lecture 5: Tutorial 1</a>
</h1>
<time datetime=2021-07-18T00:00:00Z class=post-date>Sun, Jul 18, 2021</time>
Week 1 Lecture 5: Tutorial 1 Agenda Supervised vs Unsupervised Learning Different types of Features: Categorical vs Contiious Features Supervised Learning Regression vs Classification Bias vs Varience Generalisation Performance of A Learning Algorithm Supervised vs Unsupervised Learning Basic algorithms to follow Suppose we have a huge number of images (1 Million, say)
Clustering Done -> Get a broad idea about what are the different kinds of images Classifier is run for each of these clusters to discover intricacies in this data So step 1 is unsupervised, while step 2 may be supervised or unsupervised.
<div class=read-more-link>
<a href=/MLNotes/week-1/w1lec5/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/w1lec4/>Week 1 Lecture 4: Evaluation and Cross Validation</a>
</h1>
<time datetime=2021-07-16T00:00:00Z class=post-date>Fri, Jul 16, 2021</time>
Week 1 Lecture 4: Evaluation and Cross Validation Given a hypothesis space H and training data S, the learning algo comes up with a function h. To understand how good the h is, we need to evaluate it using experimental evaluation, ie having a metric using which we evaluate, eg
error metric accuracy precision and recall These evaluations are done on the training set or even better a seperate test set.
<div class=read-more-link>
<a href=/MLNotes/week-1/w1lec4/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/w1lec3/>Week1 Lecture 3: Hypothesis Space and Inductive Bias</a>
</h1>
<time datetime=2021-07-15T00:00:00Z class=post-date>Thu, Jul 15, 2021</time>
Week1 Lecture 3: Hypothesis Space and Inductive Bias Inductive Learing or Prediction Given examples or data of form (x , y) or (x, f(x)) Classification Problems: f(x) is discrete Regression Problems: f(x) is continuous Probability Estimation: f(x) is the probability of x Why inductive learning: Given data, use induction, as opposed to deduction, to try and identify a function that predicts the data.
Features: Properties that describe each instance
<div class=read-more-link>
<a href=/MLNotes/week-1/w1lec3/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/w1lec2/>Week 1 Lecture 2: Different Types of Learning</a>
</h1>
<time datetime=2021-07-14T00:00:00Z class=post-date>Wed, Jul 14, 2021</time>
Week 1 Lecture 2: Different Types of Learning Supervised Learning Has (X, Y) given as data, where X was input, Y was output, and model tries to find out Y for a new X and compare with the given Y Give a label to X (ie find Y) UnSupervised Learning Only X is given Given X, Cluster or Summarise them, ie organise them into meaningful groups Reinforcement Learning Given an &ldquo;agent&rdquo; Determine what to do based on rewards and punishments Agent takes an action which impacts the enviornment, based on which it is rewarded ( rewards can be [-1, 0, 1]) The agent tries to optimise longterm rewards SemiSupervised Learning //Not defined in Andrew Ng&rsquo;s course, who put it in supervised learning only Combo Given labelled training data, and a even larger unlabelled data, come up with algo to process the unlabelled data Most common for now Supervised Learning We have a set of input features given \( X_1, X_2, &mldr; , X_n \), with respect to which the instances are described.
<div class=read-more-link>
<a href=/MLNotes/week-1/w1lec2/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/w1lec1/>Introduction</a>
</h1>
<time datetime=2021-07-13T00:00:00Z class=post-date>Tue, Jul 13, 2021</time>
Introduction Definition of Learning: The ability to improve behaviour or a prediction based on experience.
Building comp sys that improve with experience
Machine Learning Explores algorithms learn from data and build models from data Models can be used for some tasks, eg prediction, decision making or solving Formal definition of ML (Mitchell): A computer program (machine) is said to learn from experience E, with respect to some class of tasks T, and performance measure P, if its performance on task T as measured by P improves with experience E.
<div class=read-more-link>
<a href=/MLNotes/week-1/w1lec1/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/nn/readme/></a>
</h1>
<time datetime=0001-01-01T00:00:00Z class=post-date>Mon, Jan 1, 0001</time>
Neural Networks notes, from Various Sources Will try to list the sources here:
3b1b: http://3b1b.co/neural-networks
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/pretend/></a>
</h1>
<time datetime=0001-01-01T00:00:00Z class=post-date>Mon, Jan 1, 0001</time>
Pretend You&rsquo;re Happy Jay Foreman Bloody hell, bloody hell the world is awful Well there&rsquo;s killing and starvation And injustice and religion everywhere Everything, everything is looking dreary There&rsquo;s too many people in the world And far too much pollution in the air Everything I used to love has turned to shit All the world&rsquo;s gone bankrupt now and it Doesn&rsquo;t look like things can soon improve I&rsquo;m noticeably older than I was Definitely fatter, just because I no longer feel the need to move
<div class=read-more-link>
<a href=/MLNotes/pretend/>Read More…</a>
</div>
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/readme/>Contains notes on ML course by NPTEL at IIT-KGP</a>
</h1>
<time datetime=0001-01-01T00:00:00Z class=post-date>Mon, Jan 1, 0001</time>
Contains notes on ML course by NPTEL at IIT-KGP Course Instructor Prof. Sudeshna Saha https://yedmavus.github.io/MLNotes/category/ A latex extended notes taken by Suvam Dey
Website Credits: Soham Chakraborty and Suvam Dey
LaTeX Render Credits: Soham Chakraborty
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/readthis/>Credits</a>
</h1>
<time datetime=0001-01-01T00:00:00Z class=post-date>Mon, Jan 1, 0001</time>
First of , for credits I am extremely thankful to my friend Soham for guiding me, and laying the groundwork of LaTeX rendering. He built this page using the tutorial found here at DZHG And using the Hyde theme from here
</article><article class=post>
<h1 class=post-title>
<a href=https://yedmavus.github.io/MLNotes/week-1/readme/>README</a>
</h1>
<time datetime=0001-01-01T00:00:00Z class=post-date>Mon, Jan 1, 0001</time>
</article>
</div>
</main>
</body>
</html>