<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.119.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>K-means Clustering: My Own Insights &#183; ML Notes</title><meta name=description content><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body class="theme-base-0d layout-reverse"><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a><p class=lead>Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey</p></div><nav><ul class=sidebar-nav><li><a href=https://yedmavus.github.io/MLNotes/>Home</a></li><li><a href=https://github.com/YedMavus/>Github</a></li><li><a href=https://www.linkedin.com/in/suvam-dey/>LinkedIn</a></li><li><a href=https://yedmavus.github.io/MLNotes/category/>PageHome</a></li></ul></nav><p>&copy; 2023. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>K-means Clustering: My Own Insights</h1><time datetime=2021-09-06T00:00:00Z class=post-date>Mon, Sep 6, 2021</time><h1 id=k-means-clustering>K-means-clustering</h1><p>A cluster refers to a collection of data points aggregated together because of certain similarities.</p><p>K-Means clustering is used to clump unsupervised data into K different subgroups / clusters, in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.</p><p>The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. K-<strong>means</strong> clustering is called so, cause it finds the mean (ie trhe centroid).</p><h2 id=k-means-algorithm>K means algorithm:</h2><h3 id=classify-clusters>Classify Clusters</h3><h3 id=move-older-centroid-to-new-centroid-of-data-points>Move older centroid to new centroid of Data Points</h3><h3 id=stopping-iterations>Stopping iterations:</h3><ul><li>The centroids have stabilized â€” there is no change in their values because the clustering has been successful.</li><li>The defined number of iterations has been achieved.</li></ul><h1 id=my-idea-and-how-can-this-be-improved>My Idea, and How can this be improved:</h1><h3 id=k-means---difference-in-pattern-recognition-by-a-computer-and-our-brain>K-Means - Difference in pattern recognition by a Computer and our Brain</h3><p>What I observed is to find correlation between points in data, we look out for the gaps between clusters of data points.
i e if we inverted the colors, we look for holes in the graph (where the data was more dense).
So my idea of the algorithm is</p><ol><li>Randomly <strong>&ldquo;seed&rdquo;</strong> the data with noise (say . s) [while the data is denoted by X], such that there is less noise where there is more data. i.e. each data point has a sort of radius around which there occurs no noise.</li><li>Next we get rid of the data points (X)on the graph.</li><li>Get rid of the random noise in less dense areas and put the centroid there.</li></ol><p>This is computationally a little bit more resourceful, but it may well be much more acurate.</p></div></main></body></html>