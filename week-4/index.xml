<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week-4s on ML Notes</title><link>https://yedmavus.github.io/MLNotes/week-4/</link><description>Recent content in Week-4s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 06 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/week-4/index.xml" rel="self" type="application/rss+xml"/><item><title>K-means Clustering: My Own Insights</title><link>https://yedmavus.github.io/MLNotes/week-4/k-means-clustering/</link><pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-4/k-means-clustering/</guid><description>K-means-clustering A cluster refers to a collection of data points aggregated together because of certain similarities.
K-Means clustering is used to clump unsupervised data into K different subgroups / clusters, in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.
The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.</description></item><item><title>Week 4 Lecture 1: Bayesian Learning; An Introduction</title><link>https://yedmavus.github.io/MLNotes/week-4/w4l1/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-4/w4l1/</guid><description>Bayesian Probability Talks about probability interpretation based on partial beliefs.
Bayesian Estimation Calculates validity of a proposition: based on
Prior Estimate of its prob New relevent evidence Baye&amp;rsquo;s Theorem \( P(h|D) = \frac{P(D|h)P(h)}{P(D)} \)
How to apply this in ML? MAP [ Maximum A Posteriori] Hypothesis
\( h_{MAP} = argmax (P(h|D)) \)
\( h \epsilon H \)
\( = argmax(P(D|h)P(h)) \)
Maximum Likely [ML] Hyp: In cases where all the hypothesis are equally probable.</description></item></channel></rss>