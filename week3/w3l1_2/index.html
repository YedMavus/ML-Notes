<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.88.0">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>KNN &#183; ML Notes</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.png>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
</head>
<body class="theme-base-0d layout-reverse">
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a>
<p class=lead>
Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://yedmavus.github.io/MLNotes/>Home</a> </li>
<li><a href=https://github.com/YedMavus/> Github </a></li><li><a href=https://www.linkedin.com/in/suvam-dey/> LinkedIn </a></li><li><a href=https://yedmavus.github.io/MLNotes/category/> PageHome </a></li>
</ul>
</nav>
<p>&copy; 2021. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<h1>KNN</h1>
<time datetime=2021-08-24T00:00:00Z class=post-date>Tue, Aug 24, 2021</time>
<h2 id=feature-reduction-for-k-nearest-neighbours>Feature Reduction For K-Nearest Neighbours</h2>
<p>More features reduce accuracy. More info != more discriminative power!</p>
<p>This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn)
Moreover they may have redundant features, as we have limited computational resources.</p>
<p>So we use 2 methods to reduce this:</p>
<ol>
<li>Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects</li>
<li>Feature Extraction: Transforms or projects the existing features to a dimention m &lt; n (original no. of dimentions)</li>
</ol>
<p>Both cases we try to improve or maintain classification accuracy while simplifying it.</p>
<p>For <strong>1</strong>, we have \( 2^n \) possible subsets
so finding F' can use</p>
<ul>
<li>Optimised algo</li>
<li>Heuristic / Greedy algo</li>
<li>Randomised algo</li>
</ul>
<p>For evaluation we have 2 methods: <strong>Unsupervised</strong> (Filter methods) that looks only at the input and outputs the one with most information and <strong>Supervised methods</strong> (Wrapper methods) that actually uses it on the learning algo and estimates error on validation set.</p>
<p>Strategies</p>
<ul>
<li>Use uncorrelated features [eg Age may be directly related to one&rsquo;s weight when we consider children, so we consider only one of those features]
<ul>
<li>Forward Selection: Start from an empty set of features</li>
<li>Try each remaining feature</li>
<li>Estimate regression/classification error for each feature</li>
<li>Select feature which gives best improvement and continue this one by one, until it doesnt improve any more</li>
</ul>
</li>
<li>Backward Selection:
<ul>
<li>Start with full feature set</li>
<li>Try removing features as see how accuracy improves for a particular feature</li>
<li>Drop the feature whose removal gives least improvement / impact on error</li>
</ul>
</li>
</ul>
<p>Feature Selection can be <strong>univariate</strong> (looking at each feature one at a time) or <strong>multivariate</strong></p>
<h3 id=univariate>Univariate</h3>
<ul>
<li>Pearson correlation coefficient r \( -1 &lt; r &lt; +1 \) r = 0 implies no correlation, r = +1 or -1 implies good correlation</li>
<li>F-Score</li>
<li>Chi-Square Test</li>
<li>Signal to noise ratio: \( \frac{Difference in means}{Difference in std dev in two classes} \). Large values = good correlation</li>
<li>Mutual Information</li>
<li>etc</li>
</ul>
<h4 id=we-then-rank-the-features>We then rank the features</h4>
<h3 id=multivariate>Multivariate</h3>
<ul>
<li>Considers all features simultaneously</li>
<li>Consider \( \vec w \) for any linear classifier</li>
<li>Classifiaction of a point X is given by \( w^T x + w_0 \)</li>
<li>W is basically the weights given to each feature, and the indices can be ranked based on the weights</li>
<li>In recursive feature elimination, this is used, where features with least w are recursively removed till a reduction in accuracy is observed.</li>
</ul>
<h4 id=here--ve-or-ve-val-doesnt-matter-only-the-abs-val-determine-the-rank>Here -ve or +ve val doesnt matter. Only the abs val determine the rank.</h4>
</div>
</main>
</body>
</html>