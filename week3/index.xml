<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week3s on ML Notes</title><link>https://yedmavus.github.io/MLNotes/week3/</link><description>Recent content in Week3s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/week3/index.xml" rel="self" type="application/rss+xml"/><item><title>Collaborative Filtering</title><link>https://yedmavus.github.io/MLNotes/week3/w3l4/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l4/</guid><description>Recommender Systems Item Recomendation Rating Prediction Defn: We have a set of users U and a set of items S
P is a function from
\( P : U \bigcross S \rightarrow R \)
So we learn P from data, and based on P we predict the utility value of each item to each user.
Content based RS: Rating prediction based on content of current and previous item (like in YouTube) Collaborative or Content Based RS For a user find similar users [based on past data] and if those similar users have given a rating of an item, predict that [kind of like Knn] Item Based [Alternative to Collaborative]</description></item><item><title>Feature Extraction</title><link>https://yedmavus.github.io/MLNotes/week3/w3l3/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l3/</guid><description>We find a projection matrix \( w \) such that \( \vec z = w^T \vec X \)
Also features have to have large variences
Principal Components Find w (eigenvecotrs) to get max varience, then lecond max , 3rd max etc till N dimensions are fulfilled
More on PCA</description></item><item><title>KNN</title><link>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</guid><description>Feature Reduction For K-Nearest Neighbours More features reduce accuracy. More info != more discriminative power!
This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn) Moreover they may have redundant features, as we have limited computational resources.
So we use 2 methods to reduce this:
Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects Feature Extraction: Transforms or projects the existing features to a dimention m &amp;lt; n (original no.</description></item></channel></rss>