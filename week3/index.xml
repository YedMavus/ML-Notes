<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week3s on ML Notes</title><link>https://yedmavus.github.io/MLNotes/week3/</link><description>Recent content in Week3s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/week3/index.xml" rel="self" type="application/rss+xml"/><item><title>Feature Extraction</title><link>https://yedmavus.github.io/MLNotes/week3/w3l3/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l3/</guid><description>We find a projection matrix \( w \) such that \( \vec z = w^T \vec X \)</description></item><item><title>KNN</title><link>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</guid><description>Feature Reduction For K-Nearest Neighbours More features reduce accuracy. More info != more discriminative power!
This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn) Moreover they may have redundant features, as we have limited computational resources.
So we use 2 methods to reduce this:
Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects Feature Extraction: Transforms or projects the existing features to a dimention m &amp;lt; n (original no.</description></item></channel></rss>