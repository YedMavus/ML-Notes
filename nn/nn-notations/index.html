<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.88.1">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Notation in Neural Networks &#183; ML Notes</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.png>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
</head>
<body class="theme-base-0d layout-reverse">
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a>
<p class=lead>
Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://yedmavus.github.io/MLNotes/>Home</a> </li>
<li><a href=https://github.com/YedMavus/> Github </a></li><li><a href=https://www.linkedin.com/in/suvam-dey/> LinkedIn </a></li><li><a href=https://yedmavus.github.io/MLNotes/category/> PageHome </a></li>
</ul>
</nav>
<p>&copy; 2021. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<h1>Notation in Neural Networks</h1>
<time datetime=2021-08-17T00:00:00Z class=post-date>Tue, Aug 17, 2021</time>
<h1 id=notation-in-neural-networks>Notation in Neural Networks</h1>
<p>Reference Video: <a href=https://www.3blue1brown.com/lessons/backpropagation-calculus>https://www.3blue1brown.com/lessons/backpropagation-calculus</a></p>
<h3 id=a-usually-denotes-activation-layer-node-w---weights-b---bias>a usually denotes activation layer node, w - weights, b - bias</h3>
<p>To imply <strong>which layer</strong> the particular node is in, we use the superscript.
If the node is in layer L, it&rsquo;s activation may be denoted by \( a^{(L)} \) while that of the one before may be \( a^{(L-1)} \) .</p>
<p>If the desider output is denoted by y, for that particular activation node, it&rsquo;s cost is \( C_0 ( \cdots ) = ( a ^{(L)} - y^2 ) \)</p>
<p>Also \( a^{(L)} = \sigma ( w ^{(L)} a^{(L-1)} + b^{(L)} ) \) where b denotes the bias.</p>
<p>For simplicity, \( w ^{(L)} a^{(L-1)} + b^{(L)} \) can be denoted by \( z^{(L)} \)
so</p>
<p>\( a^{(L)} = \sigma( z^{(L)} ) \)</p>
<p>To imply which particular neuron / node of a particular layer we are referring to, we use the subscript.</p>
</div>
</main>
</body>
</html>