<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.85.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Week 1 Lecture 5: Tutorial 1 &#183; ML Notes</title><meta name=description content><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body class="theme-base-0d layout-reverse"><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a><p class=lead>Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey</p></div><nav><ul class=sidebar-nav><li><a href=https://yedmavus.github.io/MLNotes/>Home</a></li><li><a href=https://github.com/YedMavus/>Github</a></li><li><a href=https://www.linkedin.com/in/suvam-dey/>LinkedIn</a></li></ul></nav><p>&copy; 2021. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Week 1 Lecture 5: Tutorial 1</h1><time datetime=2021-07-18T00:00:00Z class=post-date>Sun, Jul 18, 2021</time><h1 id=week-1-lecture-5-tutorial-1>Week 1 Lecture 5: Tutorial 1</h1><h2 id=agenda>Agenda</h2><ul><li>Supervised vs Unsupervised Learning</li><li>Different types of Features: Categorical vs Contiious Features</li><li>Supervised Learning<ul><li>Regression vs Classification</li></ul></li><li>Bias vs Varience</li><li>Generalisation Performance of A Learning Algorithm</li></ul><h2 id=supervised-vs-unsupervised-learning>Supervised vs Unsupervised Learning</h2><p>Basic algorithms to follow
Suppose we have a huge number of images (1 Million, say)</p><ol><li>Clustering Done -> Get a broad idea about what are the different kinds of images</li><li>Classifier is run for each of these clusters to discover intricacies in this data</li></ol><p>So step 1 is unsupervised, while step 2 may be supervised or unsupervised.</p><h2 id=categorical-vs-continious>Categorical vs Continious</h2><h3 id=categorical>Categorical</h3><p>Finite number of Values, or indicating presence and absence of something</p><p>//Imp for exam purpose these categories</p><h3 id=continious>Continious</h3><p>Can theoretically take infinie number of values, eg Height, weight, price, etc</p><h2 id=types-of-supervised-learning-algo>Types of Supervised Learning Algo</h2><h5 id=dependent-on-type-of-output-variable>Dependent on type of output variable</h5><p>(Check if its discrete or continious)</p><ul><li>Regression</li><li>Given certain features of car, predict price of car</li><li>Classification</li><li>Is it categorical or discrete?</li><li>examples</li><li>given images of animals predict species of animal</li><li>given CT Scans, predict malignant tumor</li></ul><h2 id=bias-vs-variance>Bias vs Variance</h2><h4 id=bias>Bias</h4><ul><li>Set of erroneous <strong>assumptions</strong> in the learning algorithm</li><li>This is only due to the learning algo, and not the training examples</li></ul><h4 id=variance>Variance</h4><ul><li>Due to sensitivity of learning towards noise as opposed to the features and relationship of input/output</li><li>Happens if too many features are being considered</li><li>Happens if too less training data</li></ul><table><thead><tr><th></th><th>Number of Features</th><th>Number of Parameters</th><th>Number of Training Examples</th></tr></thead><tbody><tr><td><strong>Bias</strong></td><td>Decreases</td><td>Decreases</td><td>Remains the same</td></tr><tr><td><strong>Variance</strong></td><td>Increases</td><td>Increases</td><td>Decreases</td></tr></tbody></table><h2 id=generalisation-of-performance>Generalisation of Performance</h2><p>How good does the LA perform when given new training examples?
This can be controlled by controlling the bias and variance.</p></div></main></body></html>