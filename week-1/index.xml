<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week-1s on ML Notes</title><link>https://yedmavus.github.io/MLNotes/week-1/</link><description>Recent content in Week-1s on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 17 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/week-1/index.xml" rel="self" type="application/rss+xml"/><item><title>Week 1 Lecture 4: Evaluation and Cross Validation</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec4/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec4/</guid><description>Week 1 Lecture 4: Evaluation and Cross Validation Given a hypothesis space H and training data S, the learning algo comes up with a function h. To understand how good the h is, we need to evaluate it using experimental evaluation, ie having a metric using which we evaluate, eg
error metric accuracy precision and recall These evaluations are done on the training set or even better a seperate test set.</description></item><item><title>Week 1 Lecture 5: Tutorial 1</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec5/</link><pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec5/</guid><description>Week 1 Lecture 5: Tutorial 1 Supervised vs Unsupervised Learning Different types of Features: Categorical vs Contiious Features Supervised Learning Regression vs Classification Bias vs Varience Generalisation Performance of A Learning Algorithm Supervised vs Unsupervised Learning</description></item><item><title>Week1 Lecture 3: Hypothesis Space and Inductive Bias</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec3/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec3/</guid><description>Week1 Lecture 3: Hypothesis Space and Inductive Bias Inductive Learing or Prediction Given examples or data of form (x , y) / (x, f(x)) Classification Problems: f(x) is discrete Regression Problems: f(x) is continuous Probability Estimation: f(x) is the probability of x ** Why inductive learning**: Given data, use induction, as opposed to deduction, to try and identify a function that predicts the data. Features: Properties that describe each instance</description></item><item><title>Week 1 Lecture 2: Different Types of Learning</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec2/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec2/</guid><description>Week 1 Lecture 2: Different Types of Learning Supervised Learning Has (X, Y) given as data, where X was input, Y was output, and model tries to find out Y for a new X and compare with the given Y Give a label to X (ie find Y) UnSupervised Learning Only X is given Given X, Cluster or Summarise them, ie organise them into meaningful groups Reinforcement Learning Given an &amp;ldquo;agent&amp;rdquo; Determine what to do based on rewards and punishments Agent takes an action which impacts the enviornment, based on which it is rewarded ( rewards can be [-1, 0, 1]) The agent tries to optimise longterm rewards SemiSupervised Learning //Not defined in Andrew Ng&amp;rsquo;s course, who put it in supervised learning only Combo Given labelled training data, and a even larger unlabelled data, come up with algo to process the unlabelled data Most common for now Supervised Learning We have a set of input features given X1, X2, &amp;hellip; , Xn, with respect to which the instances are described.</description></item><item><title>Introduction</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec1/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec1/</guid><description>Introduction Definition of Learning: The ability to improve behaviour or a prediction based on experience.
Building comp sys that improve with experience
Machine Learning Explores algorithms learn from data and build models from data Models can be used for some tasks, eg prediction, decision making or solving Formal definition of ML (Mitchell): A computer program (machine) is said to learn from experience E, with respect to some class of tasks T, and performance measure P, if its performance on task T as measured by P improves with experience E.</description></item><item><title/><link>https://yedmavus.github.io/MLNotes/week-1/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/readme/</guid><description>Week 1 Lecture Notes</description></item></channel></rss>