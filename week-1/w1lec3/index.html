<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.85.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Week1 Lecture 3: Hypothesis Space and Inductive Bias &#183; ML Notes</title><meta name=description content><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a><p class=lead>Machine Learning Notes Based on NPTEL Lectures at IIT-Kgp | Course instructor Prof Sudeshna Saha | Notes taken by Suvam Dey</p></div><nav><ul class=sidebar-nav><li><a href=https://yedmavus.github.io/MLNotes/>Home</a></li><li><a href=https://github.com/YedMavus/>Github</a></li><li><a href=https://www.linkedin.com/in/suvam-dey/>LinkedIn</a></li></ul></nav><p>&copy; 2021. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Week1 Lecture 3: Hypothesis Space and Inductive Bias</h1><time datetime=2021-07-17T00:00:00Z class=post-date>Sat, Jul 17, 2021</time><h1 id=week1-lecture-3-hypothesis-space-and-inductive-bias>Week1 Lecture 3: Hypothesis Space and Inductive Bias</h1><h2 id=inductive-learing-or-prediction>Inductive Learing or Prediction</h2><ul><li>Given examples or data of form (x , y) / (x, f(x))<ul><li><strong>Classification Problems:</strong> f(x) is discrete</li><li><strong>Regression Problems</strong>: f(x) is continuous</li><li><strong>Probability Estimation:</strong> f(x) is the probability of x
**
Why inductive learning**: Given data, use induction, as opposed to deduction, to try and identify a function that predicts the data.</li></ul></li></ul><p>Features: Properties that describe each instance</p><p>If multiple features, use feature vector.</p><h3 id=feature-space>Feature Space</h3><p>Recall Vector Space</p><p>For n features, we define a n fimentional &ldquo;feature&rdquo; space, similar to vector spaces. Each instance becomes a point in the feature space.</p><p>Take a 2 class classification problem.
We have 2 types of instances.
Training set has subset of instances - some marked class 1{denoted by +1} , others 2{denoted by -1}.</p><p>We can map different points in the feature set. We want a function to predict, for a new instance, whether it is a + or - .
It could be a curve or line that seperates an area in the feature space (2D, so area). This is what inductive learning is. Points to the left of this line is positive, to the right are negative, for example. The line is a hypothesis that we use to do the prediction.
It needn&rsquo;t be a linear function. It could be a polynomial curve, or even a decision tree. These are just representations. Neural Networks are also another representation.</p><p><strong>Hypothesis space</strong>: Class of functions that can in principle be output by the learning algorithm. It is represented by H. Output of a learning algo produces h âˆˆ H.</p><ul><li>Supervised learning methods kind of explores the hypothesis space.</li></ul><p>The Hypothesis h can be biased, ie restricts the hypothesis using</p><ul><li>Constraints</li><li>Preferences</li></ul><h4 id=if-there-are-n-input-features-there-are-2supnsup-possible-boolean-functions-each-of-which-is-iterated-by-the-learner-to-figure-out-the-best-possible-approach>If there are N input features, there are 2N possible boolean functions, each of which is iterated by the learner to figure out the best possible approach.</h4><p>{Derived from simple combinatorics}</p><h3 id=inductive-bias>Inductive Bias</h3><ul><li>Restrictive: Limit on Hypothesis space - specifying the form of the function [eg we state that we only look at linear funtions]</li><li>Preferencial: Ordering is imposed on Hyp space - eg we specify that we prefer a function of lower degree, even though we consider all possible functions</li></ul><p>Inductive learning comes up with a general fucntion from the training examples.</p><ul><li>Constructs hypothesis h to agree on c - the training example</li><li>h is said to be consistent if it agrees with all training examples [not always possible, or may be more than one]</li><li>If it works on unseen data as well is said to generalise well.</li></ul><p>Inductive learning is also, quite ill posed - wastes resources by looking at mostly wrong hypotheses.</p><h2 id=errors>Errors</h2><p>Bias errrors [preference in choosing a hyp thaqt was incorrect] and Varience errors [ wehn we have a small test set, so the model is inconsistent with other training set hypotheses&mldr;will be discussed later] These two are basically stating overfitting and under fitting with lot of gargantuan words</p><h1 id=tldr>TLDR</h1></div></main></body></html>