<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us>
<head>
<link href=https://gmpg.org/xfn/11 rel=profile>
<meta charset=utf-8>
<meta name=generator content="Hugo 0.88.0">
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Week 1 Lecture 2: Different Types of Learning &#183; ML Notes</title>
<meta name=description content>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css>
<link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png>
<link rel="shortcut icon" href=/favicon.png>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
</head>
<body class="theme-base-0d layout-reverse">
<aside class=sidebar>
<div class="container sidebar-sticky">
<div class=sidebar-about>
<a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a>
<p class=lead>
Machine Learning Notes Based on Andrew Ng's ML lectures and NPTEL Lectures at IIT-Kgp | Course instructor Prof Ng and Prof Sudeshna Saha | Notes taken by Suvam Dey
</p>
</div>
<nav>
<ul class=sidebar-nav>
<li><a href=https://yedmavus.github.io/MLNotes/>Home</a> </li>
<li><a href=https://github.com/YedMavus/> Github </a></li><li><a href=https://www.linkedin.com/in/suvam-dey/> LinkedIn </a></li><li><a href=https://yedmavus.github.io/MLNotes/category/> PageHome </a></li>
</ul>
</nav>
<p>&copy; 2021. All rights reserved. </p>
</div>
</aside>
<main class="content container">
<div class=post>
<h1>Week 1 Lecture 2: Different Types of Learning</h1>
<time datetime=2021-07-14T00:00:00Z class=post-date>Wed, Jul 14, 2021</time>
<h1 id=week-1-lecture-2-different-types-of-learning>Week 1 Lecture 2: Different Types of Learning</h1>
<ul>
<li>Supervised Learning
<ul>
<li>Has (X, Y) given as data, where X was input, Y was output, and model tries to find out Y for a new X and compare with the given Y</li>
<li>Give a label to X (ie find Y)</li>
</ul>
</li>
</ul>
<ul>
<li>UnSupervised Learning
<ul>
<li>Only X is given</li>
<li>Given X, Cluster or Summarise them, ie organise them into meaningful groups</li>
</ul>
</li>
<li>Reinforcement Learning
<ul>
<li>Given an &ldquo;agent&rdquo; Determine what to do based on rewards and punishments</li>
<li>Agent takes an action which impacts the enviornment, based on which it is rewarded ( rewards can be [-1, 0, 1])</li>
<li>The agent tries to optimise longterm rewards</li>
</ul>
</li>
<li>SemiSupervised Learning //Not defined in Andrew Ng&rsquo;s course, who put it in supervised learning only
<ul>
<li>Combo</li>
<li>Given labelled training data, and a even larger unlabelled data, come up with algo to process the unlabelled data</li>
<li>Most common for now</li>
</ul>
</li>
</ul>
<h2 id=supervised-learning>Supervised Learning</h2>
<p>We have a set of input features given \( X_1, X_2, &mldr; , X_n \), with respect to which the instances are described. We also have a target feature Y.</p>
<table>
<thead>
<tr>
<th>S.No</th>
<th>\( X_1, X_2, &mldr; , X_n \)</th>
<th>Y</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.</td>
<td>\( A_1, A_2, &mldr; , A_n \)</td>
<td>\( Y_1\)</td>
</tr>
<tr>
<td>2.</td>
<td>\( B_1, B_2, &mldr; , B_n \)</td>
<td>\( Y_2\)</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
</tr>
</tbody>
</table>
<p>So the values of \( Y_1\) are given. Along with this a test instance is given, containing only \( X_1\).</p>
<ul>
<li>If Y is discrete valued, [eg Will it rain, or not rain] it is known as <strong>Classification</strong>.</li>
<li>If Y is continious valued, [eg Given a location, predict price of house per sq inches], it is known as <strong>Regression</strong>.
<ul>
<li>Most common example - linear regression</li>
</ul>
</li>
</ul>
<h3 id=features>Features</h3>
<p>Individual observations are analysed into a set of <strong>quantifiable</strong> properties which are called features
eg Type of blood group, or Ordinal [small, medium or large] etc</p>
<h4 id=to-summarise-supervised-learning>To Summarise <strong>Supervised Learning</strong></h4>
<table>
<thead>
<tr>
<th></th>
<th>Traning Set</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Learning Algo</td>
<td></td>
</tr>
<tr>
<td><strong>X</strong></td>
<td>Hypothesis</td>
<td>Predicted Y</td>
</tr>
</tbody>
</table>
<h2 id=classification-learning>Classification Learning</h2>
<ul>
<li>Task T is given
<ul>
<li>Input: Set of Instances \( d_1, d_2, &mldr; , d_n \)
<ul>
<li>Each instance has a set of features</li>
<li>So d can be represented as a vector <strong>\vec d</strong> = &lt; \( X_1, X_2, &mldr; , X_n \)></li>
</ul>
</li>
<li>Output: A set of predictions as one of a fixed set of constant values</li>
</ul>
</li>
<li>Performance Metric P
<ul>
<li>Probability of a prediction being wrong</li>
</ul>
</li>
<li>Experience E
<ul>
<li>It is the data, ie a given set of correctly labelled examples (X,Y), <code>coming from a fixed distribution</code> (ideally)</li>
</ul>
</li>
</ul>
<h2 id=tldr>TLDR</h2>
<ul>
<li><strong>Features:</strong> Distinct traits that is used to describe a particular instance quantitatively</li>
<li><strong>Feature Vector:</strong> n - dimentional vector of numerical features</li>
<li><strong>Instance space X:</strong> Set of all possible objects describeable by features</li>
<li><strong>Example (x, y):</strong> Instance x with label f(x) = y</li>
<li><strong>Concept c:</strong> Subset of objects from X [eg given a group of images, those images that are of a bicycle are under a concept c that contains images of bicycles]</li>
<li><strong>Target Function:</strong> Maps each instance x ε X to target label y ε Y</li>
<li><strong>Training Data:</strong> Collection of examples observed by learning algorithm, used to potentially predict relationships [in the Computer Science sense ;) ]</li>
</ul>
</div>
</main>
</body>
</html>