<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.85.0"><meta name=viewport content="width=device-width,initial-scale=1"><title>Week 1 Lecture 4: Evaluation and Cross Validation &#183; ML Notes</title><meta name=description content><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/print.css media=print><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/poole.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/syntax.css><link type=text/css rel=stylesheet href=https://yedmavus.github.io/MLNotes/css/hyde.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://yedmavus.github.io/MLNotes/><h1>ML Notes</h1></a><p class=lead>Machine Learning Notes Based on NPTEL Lectures at IIT-Kgp | Course instructor Prof Sudeshna Saha | Notes taken by Suvam Dey</p></div><nav><ul class=sidebar-nav><li><a href=https://yedmavus.github.io/MLNotes/>Home</a></li><li><a href=https://github.com/YedMavus/>Github</a></li><li><a href=https://www.linkedin.com/in/suvam-dey/>LinkedIn</a></li></ul></nav><p>&copy; 2021. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Week 1 Lecture 4: Evaluation and Cross Validation</h1><time datetime=2021-07-17T00:00:00Z class=post-date>Sat, Jul 17, 2021</time><h1 id=week-1-lecture-4-evaluation-and-cross-validation>Week 1 Lecture 4: Evaluation and Cross Validation</h1><p>Given a hypothesis space H and training data S, the learning algo comes up with a function h. To understand how good the h is, we need to evaluate it using experimental evaluation, ie having a metric using which we evaluate, eg</p><ul><li>error metric</li><li>accuracy</li><li>precision and recall</li></ul><p>These evaluations are done on the training set or even better a seperate test set.</p><p>Given y' = h(x) is a prediction on x and y is the actual value. If y' differs from y, we have an error.</p><h4 id=types-of-errors>Types of errors:</h4><ul><li>Absoloute error: 1/n ∑{|h(x) - y|}</li><li>Sum of Square Method: 1/n ∑(h(x) - y)2</li><li>Classification error: 1/n ∑ δ (h(x),y)</li></ul><p>In classification, we define a confusion matrix</p><table><thead><tr><th>Hyp Class\True Class</th><th>Positive</th><th>Negative</th></tr></thead><tbody><tr><td>Positive</td><td>True Positive[TP]</td><td>False Positive[FP]</td></tr><tr><td>Negative</td><td>False Negative[FN]</td><td>True Negative[TN]</td></tr><tr><td></td><td>∑ = P</td><td>∑ = N</td></tr></tbody></table><p>Obviously the diagonal elements are either all true or all false</p><h4 id=accuracy--tp--tn--p--n>Accuracy = (TP + TN) / (P + N)</h4><p>\How many are correctly predicted</p><h4 id=precision--tp-tp--fp>Precision = TP/ (TP + FP)</h4><p>\Answers how many are correctly positive</p><h4 id=recall--tp--p>Recall = TP / P</h4><p>How many of the positive examples are retrieved as positive, also called true positive rate. False positive rate also exists.</p><p>Error Got on Sample is called <strong>sample error</strong>. The actual error is called the <strong>true error</strong>.</p><p>We split the example dataset, using a part for training the learner, and a disjoint dataset for testing the learner.</p><p>If testset is small, the varience increases.</p><h4 id=how-to-test-with-limited-data>How to test with limited data</h4><ul><li>Divide the examples into training set and test sets.<ul><li>The size of training set incerases, then overfitting occurs, so we prefer to give max of the data for training.</li></ul></li><li>During traning, a small part of the training data can be used as validation set, if training is sort of ok. The validation set is used to do minor tweaks, so to say.</li></ul><h3 id=k---fold-cross-validation>K - Fold cross validation</h3><ol><li>Split Data into K Equal subsets</li><li>Perform K Rounds of learning. On each round</li></ol><ul><li>1/k of the data is held out as test set</li><li>Remaining examples used as training data</li></ul><ol start=3><li>Compute average test set score of K Rounds</li></ol><p>Final accuracy is given by average of each of these K Subsets</p><h2 id=trade-off-tldr>Trade off [TLDR]</h2><ul><li>Always a tradeoff between complex hyp that fits training data well</li><li>Simpler hypothesis that generalise better<ul><li>As data amount increases, generalisation error increases</li></ul></li></ul></div></main></body></html>