<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML Notes</title><link>https://yedmavus.github.io/MLNotes/</link><description>Recent content on ML Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://yedmavus.github.io/MLNotes/index.xml" rel="self" type="application/rss+xml"/><item><title>Feature Extraction</title><link>https://yedmavus.github.io/MLNotes/week3/w3l3/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l3/</guid><description>We find a projection matrix \( w \) such that \( \vec z = w^T \vec X \)</description></item><item><title>KNN</title><link>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</link><pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week3/w3l1_2/</guid><description>Feature Reduction For K-Nearest Neighbours More features reduce accuracy. More info != more discriminative power!
This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn) Moreover they may have redundant features, as we have limited computational resources.
So we use 2 methods to reduce this:
Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects Feature Extraction: Transforms or projects the existing features to a dimention m &amp;lt; n (original no.</description></item><item><title>Note Home</title><link>https://yedmavus.github.io/MLNotes/category/</link><pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/category/</guid><description> Weekly Progression:
Week-1 Lecture 1 Week-1 Lecture 2 Week-1 Lecture 3 Week-1 Lecture 4 Week-1 Lecture 5 Week-2 Lecture 1 Week-2 Lecture 2 Week-2 Lecture 3 Week-3 Lecture 1,2 NeuralNetworks
Blue1Brown-1 Notations in Neural Networks Supremely Better Resources than this crap:
http://3b1b.co/neural-networks http://colah.github.io/ http://neuralnetworksanddeeplearning.com/chap1.html</description></item><item><title>Notation in Neural Networks</title><link>https://yedmavus.github.io/MLNotes/nn/nn-notations/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/nn/nn-notations/</guid><description>Notation in Neural Networks Reference Video: https://www.3blue1brown.com/lessons/backpropagation-calculus
a usually denotes activation layer node, w - weights, b - bias To imply which layer the particular node is in, we use the superscript. If the node is in layer L, it&amp;rsquo;s activation may be denoted by \( a^{(L)} \) while that of the one before may be \( a^{(L-1)} \) .
If the desider output is denoted by y, for that particular activation node, it&amp;rsquo;s cost is \( C_0 ( \cdots ) = ( a ^{(L)} - y^2 ) \)</description></item><item><title>3b1bNN1</title><link>https://yedmavus.github.io/MLNotes/nn/3b1bnn1/</link><pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/nn/3b1bnn1/</guid><description>But What is a Neural Network Here we take example of handwriting recognition to detect Numbers Couple of layers Each layer contains nodes that take care of a part of the number. So each of these functions sort fo drives or pushes the output towards on enumber, which on adding up end up predicting one particular number at the end.
Basically it is calculating weighted sum, while squishing the whole number line in between 0-1, so that the numbers are easier to process, and is anologous to probability.</description></item><item><title>Week 2 Lecture 3: Learning Decision Tree</title><link>https://yedmavus.github.io/MLNotes/week-2/w2l3/</link><pubDate>Sat, 07 Aug 2021 00:03:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-2/w2l3/</guid><description>Week 2 Lecture 3: Learning Decision Tree https://www.kaggle.com/dansbecker/underfitting-and-overfitting</description></item><item><title>Week 2 Lecture 2: Decision Tree</title><link>https://yedmavus.github.io/MLNotes/week-2/w2l2/</link><pubDate>Mon, 02 Aug 2021 00:03:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-2/w2l2/</guid><description>Week 2 Lecture 2: Decision Tree Decision Trees Its a tree structured classifier consisting of
Decision nodes - specify a choice or a test that tells you to which subsequent branch to take Has 2, or even 3 branches Leaf Nodes - dont have any subsequent chain(s), indicates the classification or value of example Used mainly to visualise algorithms.</description></item><item><title>Week 2 Lecture 1: Linear Regression</title><link>https://yedmavus.github.io/MLNotes/week-2/w2l1/</link><pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-2/w2l1/</guid><description>Week 2 Lecture 1: Linear Regression Regression in X and Y Firstly for regression Y is continious X \( \rightarrow \) Y
Single Regression: X consists of only 1 feature Find fucntion given arbitrary X, that can predict Y with minimum error In Linear Regression this output Y function happens to be a line, trained based on an existing set of (X,Y) Multiple Regression: Multiple features in X Linear Regression Simple Linear Regression: \( Y = \theta _0 + \theta _ 1 X \) Obviously, \( \theta _0 \) is the Y axis intercept, while \( \theta _ 1 \) is the population slope.</description></item><item><title>Week 1 Lecture 5: Tutorial 1</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec5/</link><pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec5/</guid><description>Week 1 Lecture 5: Tutorial 1 Agenda Supervised vs Unsupervised Learning Different types of Features: Categorical vs Contiious Features Supervised Learning Regression vs Classification Bias vs Varience Generalisation Performance of A Learning Algorithm Supervised vs Unsupervised Learning Basic algorithms to follow Suppose we have a huge number of images (1 Million, say)
Clustering Done -&amp;gt; Get a broad idea about what are the different kinds of images Classifier is run for each of these clusters to discover intricacies in this data So step 1 is unsupervised, while step 2 may be supervised or unsupervised.</description></item><item><title>Week 1 Lecture 4: Evaluation and Cross Validation</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec4/</link><pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec4/</guid><description>Week 1 Lecture 4: Evaluation and Cross Validation Given a hypothesis space H and training data S, the learning algo comes up with a function h. To understand how good the h is, we need to evaluate it using experimental evaluation, ie having a metric using which we evaluate, eg
error metric accuracy precision and recall These evaluations are done on the training set or even better a seperate test set.</description></item><item><title>Week1 Lecture 3: Hypothesis Space and Inductive Bias</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec3/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec3/</guid><description>Week1 Lecture 3: Hypothesis Space and Inductive Bias Inductive Learing or Prediction Given examples or data of form (x , y) or (x, f(x)) Classification Problems: f(x) is discrete Regression Problems: f(x) is continuous Probability Estimation: f(x) is the probability of x Why inductive learning: Given data, use induction, as opposed to deduction, to try and identify a function that predicts the data.
Features: Properties that describe each instance</description></item><item><title>Week 1 Lecture 2: Different Types of Learning</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec2/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec2/</guid><description>Week 1 Lecture 2: Different Types of Learning Supervised Learning Has (X, Y) given as data, where X was input, Y was output, and model tries to find out Y for a new X and compare with the given Y Give a label to X (ie find Y) UnSupervised Learning Only X is given Given X, Cluster or Summarise them, ie organise them into meaningful groups Reinforcement Learning Given an &amp;ldquo;agent&amp;rdquo; Determine what to do based on rewards and punishments Agent takes an action which impacts the enviornment, based on which it is rewarded ( rewards can be [-1, 0, 1]) The agent tries to optimise longterm rewards SemiSupervised Learning //Not defined in Andrew Ng&amp;rsquo;s course, who put it in supervised learning only Combo Given labelled training data, and a even larger unlabelled data, come up with algo to process the unlabelled data Most common for now Supervised Learning We have a set of input features given \( X_1, X_2, &amp;hellip; , X_n \), with respect to which the instances are described.</description></item><item><title>Introduction</title><link>https://yedmavus.github.io/MLNotes/week-1/w1lec1/</link><pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/w1lec1/</guid><description>Introduction Definition of Learning: The ability to improve behaviour or a prediction based on experience.
Building comp sys that improve with experience
Machine Learning Explores algorithms learn from data and build models from data Models can be used for some tasks, eg prediction, decision making or solving Formal definition of ML (Mitchell): A computer program (machine) is said to learn from experience E, with respect to some class of tasks T, and performance measure P, if its performance on task T as measured by P improves with experience E.</description></item><item><title/><link>https://yedmavus.github.io/MLNotes/nn/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/nn/readme/</guid><description>Neural Networks notes, from Various Sources Will try to list the sources here:
3b1b: http://3b1b.co/neural-networks</description></item><item><title>Contains notes on ML course by NPTEL at IIT-KGP</title><link>https://yedmavus.github.io/MLNotes/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/readme/</guid><description>Contains notes on ML course by NPTEL at IIT-KGP Course Instructor Prof. Sudeshna Saha https://yedmavus.github.io/MLNotes/category/ A latex extended notes taken by Suvam Dey
Website Credits: Soham Chakraborty and Suvam Dey
LaTeX Render Credits: Soham Chakraborty</description></item><item><title>Credits</title><link>https://yedmavus.github.io/MLNotes/readthis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/readthis/</guid><description>First of , for credits I am extremely thankful to my friend Soham for guiding me, and laying the groundwork of LaTeX rendering. He built this page using the tutorial found here at DZHG And using the Hyde theme from here</description></item><item><title>README</title><link>https://yedmavus.github.io/MLNotes/week-1/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://yedmavus.github.io/MLNotes/week-1/readme/</guid><description/></item></channel></rss>