---
title: "KNN"
date: 2021-08-24 00:00:00 +0000
katex: true
---

## Feature Reduction For K-Nearest Neighbours

More features reduce accuracy. More info != more discriminative power!

This is because, some features are irrelevant, and introduce noise, and fool the algorithm (especially a lazy algo like Knn)
Moreover they may have redundant features, as we have limited computational resources.

So we use 2 methods to reduce this:

- Feature Selection: F is fiven, find a subset F' which has elements less than F, such that it optimises cetrain aspects
- Feature Extraction: Transforms or projects the existing features to a dimention m < n (original no. of dimentions)

Both cases we try to improve or maintain classification accuracy while simplifying it.

