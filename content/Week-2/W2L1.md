---
title: "Week 2 Lecture 2: Linear Regression"
date: 2021-08-02 00:00:00 +0000
katex: true
---
\\( \usepackage{graphicx} \\)
\\( \graphicspath{ {./images/} } \\)
# Week 2 Lecture 2: Linear Regression

## Regression in X and Y

### Firstly for **regression** Y is continious

X \\( \rightarrow \\) Y

* Single Regression: X consists of only 1 feature
  * Find fucntion given arbitrary X, that can predict Y with minimum error
  * In **Linear Regression** this output Y function happens to be a line, trained based on an existing set of (X,Y)
* Multiple Regression: Multiple features in **X**

## Linear Regression

### Simple Linear Regression:
#### \\( Y = \beta _0 + \beta _ 1 X \\)

Obviously, \\( \beta _0 \\) is the Y axis intercept, while \\( \beta _ 1 \\) is the population slope.
 
### Multiple Linear Regression

#### \\( Y = \beta _0 + \beta _ 1 X + \beta _ 2 X_2 + ... + \beta _ p X_p \\)

Thus \\( \beta _i \\) where i>0 is the slope of the i^th variable X_i, and \\( \beta _0 \\) is the average value of Y if all X are made 0.


These are the expected values, in actuality, both LR in single and multiple have an extra random error term \\( \epsilon \\) added at the end.

We try to find \\( \beta _0 , \beta _ 1 , \beta _ 2 , ... , \beta _ p  \\) such that it fits the given data closely.

This is done using
